{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import   model_forward, conv_forward, backward,conv_backward\n",
    "from load_weights import initilization\n",
    "from train import  make_datasets,update_params,predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X_train shape is :  (1000, 8, 8)\nY_train shape is :  (1000, 1)\n"
    }
   ],
   "source": [
    "X,Y = make_datasets() # get dataset from train\n",
    "print('X_train shape is : ',X.shape)\n",
    "print('Y_train shape is : ',Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y_pred,Y):\n",
    "    c = (1/Y.shape[0]) * np.sum(Y * np.log(Y_pred) + (1-Y) * np.log(1-Y_pred))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,learning_rate,num_iter):\n",
    "   \n",
    "    params = initilization()\n",
    "    for i in range(num_iter):\n",
    "        #conv layer forward\n",
    "        Z1,cache1 = conv_forward(X,params[\"W1\"],params[\"b1\"],stride=1,pad=2)\n",
    "        Z2,cache2 = conv_forward(Z1,params[\"W2\"],params[\"b2\"],stride=1,pad=2)\n",
    "\n",
    "        #input for dense layer\n",
    "        Z2_flatten = np.reshape(Z2,newshape=(Z2.shape[0],-1))\n",
    "\n",
    "        #dense layer forward\n",
    "        cache3 = model_forward(Z2_flatten,params)\n",
    "        loss = cost(cache3['A4'],Y)\n",
    "\n",
    "        #backprop for dense\n",
    "        grad,dZ2 = backward(Y,Z2_flatten, params,cache3)\n",
    "\n",
    "        #backprop for cnn\n",
    "        dZ2 = np.reshape(dZ2,(dZ2.shape[0],12,12))\n",
    "        dA_prev2, grad['dW2'], grad['db2'] = conv_backward(dZ2,params[\"W2\"],params[\"b2\"],Z1,stride=1,pad=2)\n",
    "        dA_prev1, grad['dW1'], grad['db1']  = conv_backward(dA_prev2,params[\"W1\"],params[\"b1\"],X,stride=1,pad=2)\n",
    "        \n",
    "        params = update_params(params, grad, learning_rate)\n",
    "        print(loss)\n",
    "    return params\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset in standarization\n",
    "# Min-Max scaling\n",
    "X_train = (X- X.min()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-0.807022454662088\n-0.7933197697885134\n-0.7815000106879193\n-0.7718841006651914\n-0.7643205225311721\n-0.7584527486965934\n-0.753905391321098\n-0.7503604989268841\n-0.7475716171353729\n-0.7453545889499622\n-0.7435734566284098\n-0.7421278240561405\n-0.7409430807538163\n-0.7399632592839646\n-0.7391459310670985\n-0.7384585803005758\n-0.737876021905993\n-0.7373785506452474\n-0.7369506025110614\n-0.7365797768784946\n-0.7362561146713765\n-0.7359715598508589\n-0.735719553445203\n-0.7354947243607959\n-0.7352926515702323\n-0.7351096794650127\n-0.7349427731952334\n-0.7347894043732385\n-0.7346474600492907\n-0.734515169685006\n-0.7343910461667456\n-0.7342738378626175\n-0.7341624894347684\n-0.73405610964439\n-0.7339539447804523\n-0.7338553566402543\n-0.733759804215955\n-0.7336668284146586\n-0.7335760392737313\n-0.7334871052375304\n-0.7333997441437998\n-0.7333137156329091\n-0.7332288147448504\n-0.7331448665103902\n-0.7330617213762853\n-0.7329792513316468\n-0.7328973466247607\n-0.7328159129778765\n-0.7327348692225016\n-0.7326541452901592\n-0.7325736805038933\n-0.7324934221244048\n-0.7324133241118899\n-0.7323333460706726\n-0.7322534523487827\n-0.7321736112688798\n-0.7320937944705033\n-0.7320139763466605\n-0.731934133560306\n-0.7318542446284253\n-0.731774289563269\n-0.73169424956181\n-0.7316141067358182\n-0.7315338438760418\n-0.7314534442449229\n-0.7313728913930603\n-0.7312921689953062\n-0.7312112607029324\n-0.7311301500087816\n-0.7310488201227134\n-0.7309672538549716\n-0.7308854335053818\n-0.7308033407565026\n-0.7307209565690298\n-0.7306382610778953\n-0.730555233487602\n-0.7304718519654136\n-0.7303880935310612\n-0.730303933941646\n-0.7302193475704134\n-0.7301343072780356\n-0.7300487842749949\n-0.7299627479735616\n-0.7298761658277669\n-0.7297890031596246\n-0.7297012229696891\n-0.7296127857298349\n-0.7295236491559028\n-0.7294337679575674\n-0.7293430935624485\n-0.7292515738110965\n-0.7291591526190118\n-0.7290657696013261\n-0.7289713596551346\n-0.7288758524937209\n-0.7287791721260478\n-0.7286812362738507\n-0.7285819557174591\n-0.7284812335600311\n-0.7283789643981801\n-0.7282750333849488\n-0.7281693151686648\n-0.7280616726883183\n-0.7279519558026374\n-0.7278399997258476\n-0.7277256232380651\n-0.7276086266321397\n-0.7274887893513206\n-0.7273658672630111\n-0.727239589502714\n-0.7271096548085133\n-0.7269757272494282\n-0.7268374312298332\n-0.7266943456257487\n-0.7265459968757134\n-0.7263918508072321\n-0.726231302926948\n-0.7260636668353873\n-0.7258881603409535\n-0.7257038887369004\n-0.7255098245613334\n-0.7253047829731708\n-0.7250873916318041\n-0.7248560536450562\n-0.724608901721544\n-0.7243437410921317\n-0.7240579779987499\n-0.7237485295156936\n-0.7234117090694518\n-0.7230430801223456\n-0.7226372678988471\n-0.722187715518608\n-0.7216863661512362\n-0.7211232464760979\n-0.720485918509726\n-0.7197587567366343\n-0.7189219963415748\n-0.7179504894666929\n-0.7168121087366471\n-0.7154657723220493\n-0.7138591800990408\n-0.7119266417080915\n-0.7095880098778151\n-0.7067509028859218\n-0.703320032247685\n-0.6992181996653042\n-0.6944191455075339\n-0.6889775838711997\n-0.6830257072235397\n-0.6767264060366762\n-0.670228624220584\n-0.6636558633472197\n-0.6571058649638626\n-0.6506620077678005\n-0.6444081255712989\n-0.6384255276543607\n-0.6327840456534595\n-0.6275377037933979\n-0.6227220516268356\n-0.618353686165668\n-0.6144317501125878\n-0.6109408997782365\n-0.6078549674390826\n-0.6051407183016179\n-0.6027611698972921\n-0.6006783211110331\n-0.5988550941180587\n-0.5972567365752564\n-0.5958514861490214\n-0.5946111282098645\n-0.5935107222543172\n-0.5925290591283406\n-0.5916473470698895\n-0.5908508380401104\n-0.5901248954486239\n-0.5894614408169514\n-0.5888457281768323\n-0.5882809886811187\n-0.5877395780487567\n-0.5872585232330577\n-0.5867489266363739\n-0.5863779793334016\n-0.5857959715904957\n-0.5857148420327719\n-0.5846715808114129\n-0.5857724586133384\n-0.5827995235447859\n-0.5902312563372982\n-0.5828129646030283\n-0.633010455680711\n-0.6070359626909609\n-0.6787489308227557\n-0.63564758688117\n-0.61706112684146\n-0.6122490246537092\n-0.6047923008195425\n-0.6018547524896907\n-0.5956750551666666\n-0.5955536443074979\n-0.5890026430152835\n-0.594485402669809\n-0.5850632730687801\n-0.6072412813758736\n-0.5899595916903336\n-0.6412822435810531\n-0.5977143680529304\n-0.6080454734531838\n-0.5920674136364602\n-0.6077231352014019\n-0.5896294288167062\n-0.6138408462944362\n-0.5901800220967778\n-0.6166166668949934\n-0.5905624756390528\n-0.6123129379184926\n-0.5893191909419435\n-0.6100908642785284\n-0.5880425926326037\n-0.6095484327639771\n-0.5871846621588473\n-0.6091787393104064\n-0.5865205329791741\n-0.6083334026239218\n-0.5858130830205602\n-0.6073063340281949\n-0.5850565836833544\n-0.6064021994945947\n-0.584324287595437\n-0.6056290153386668\n-0.5836423097245748\n-0.604896000968369\n-0.5829958350765715\n-0.6041615311664446\n-0.5823686164637293\n-0.6034358768450118\n-0.5817571874112725\n-0.6027356197828874\n-0.5811641216139007\n-0.6020636438806457\n-0.580590611170928\n-0.6014147968771042\n-0.5800353501207167\n-0.6007846456748156\n-0.5794963884213813\n-0.6001717700635275\n-0.5789723593221243\n-0.5995760736924552\n-0.5784624676567767\n-0.5989972099815122\n-0.5779660780789773\n-0.5984343239751226\n-0.5774825028878746\n-0.5978864217551064\n-0.5770110208748976\n-0.5973526458010895\n-0.5765509526365049\n-0.5968323004418233\n-0.5761016915433043\n-0.5963247728906008\n-0.5756626926304227\n-0.5958294776181036\n-0.5752334529739861\n-0.5953458478289964\n-0.5748135013219442\n-0.5948733464751135\n-0.5744023960922392\n-0.5944114734853926\n-0.5739997254500131\n-0.5939597645745531\n-0.5736051060751033\n-0.593517786408805\n-0.5732181806453377\n-0.5930851324591702\n-0.5728386151360783\n-0.5926614205968306\n-0.5724660965781148\n-0.5922462916580074\n-0.5721003312937509\n-0.5918394081463494\n-0.571741043400204\n-0.5914404527937376\n-0.5713879734296431\n-0.5910491270662717\n-0.5710408770290085\n-0.5906651497554097\n-0.570699523756299\n-0.5902882557104843\n-0.5703636959881847\n-0.5899181946980858\n-0.5700331879359636\n-0.5895547303554665\n-0.569707804756559\n-0.5891976392165423\n-0.5693873617449118\n-0.5888467098030067\n-0.5690716835978314\n-0.5885017417790568\n-0.568760603742469\n-0.5881625451680259\n-0.5684539637239169\n-0.5878289396274003\n-0.5681516126467343\n-0.5875007537777115\n-0.5678534066653084\n-0.5871778245809823\n-0.5675592085182832\n-0.5868599967649607\n-0.5672688871027444\n-0.5865471222899451\n-0.5669823170843271\n-0.5862390598552948\n-0.5666993785398323\n-0.5859356744429115\n-0.5664199566292782\n-0.5856368368951331\n-0.5661439412946113\n-0.5853424235246085\n-0.5658712269825642\n-0.5850523157538917\n-0.5656017123893885\n-0.5847663997826238\n-0.565335300225413\n-0.5844845662803461\n-0.5650718969975848\n-0.5842067101030821\n-0.5648114128083268\n-0.5839327300319578\n-0.5645537611692116\n-0.5836625285322325\n-0.5642988588281005\n-0.5833960115312319\n-0.5640466256085281\n-0.5831330882137344\n-0.5637969842602311\n-0.5828736708335073\n-0.5635498603198342\n-0.5826176745397127\n-0.5633051819807929\n-0.582365017217043\n-0.5630628799717924\n-0.582115619338485\n-0.5628228874428745\n-0.5818694038296985\n-0.562585139858634\n-0.5816262959440689\n-0.5623495748978973\n-0.5813862231475442\n-0.5621161323593444\n-0.5811491150124489\n-0.5618847540725946\n-0.5809149031195122\n-0.5616553838143206\n-0.5806835209673988\n-0.561427967228995\n-0.5804549038891087\n-0.561202451753916\n-0.5802289889746283\n-0.5609787865481853\n-0.5800057149992834\n-0.5607569224253517\n-0.5797850223572937\n-0.560536811789452\n-0.5795668530000387\n-0.5603184085742096\n-0.5793511503786226\n-0.5601016681851727\n-0.5791378593903316\n-0.5598865474445966\n-0.5789269263286312\n-0.5596730045388842\n-0.5787182988363649\n-0.5594609989684253\n-0.5785119258618566\n-0.5592504914996791\n-0.5783077576176497\n-0.5590414441193703\n-0.578105745541625\n-0.5588338199906632\n-0.5779058422602763\n-0.5586275834112084\n-0.5777080015539395\n-0.5584226997729476\n-0.5775121783237993\n-0.5582191355235856\n-0.5773183285604896\n-0.5580168581296359\n-0.5771264093141554\n-0.5578158360409577\n-0.57693637866584\n-0.5576160386567095\n-0.5767481957000657\n-0.5574174362926436\n-0.5765618204785208\n-0.5572200001496823\n-0.5763772140147386\n-0.5570237022837071\n-0.5761943382496917\n-0.5568285155765077\n-0.5760131560282296\n-0.5566344137078365\n-0.57583363107629\n-0.5564413711285197\n-0.5756557279788154\n-0.5562493630345736\n-0.5754794121583412\n-0.5560583653422866\n-0.5753046498541899\n-0.5558683546642247\n-0.5751314081022458\n-0.5556793082861166\n-0.5749596547152676\n-0.5554912041445902\n-0.5747893582637059\n-0.5553040208057152\n-0.5746204880570045\n-0.5551177374443274\n-0.5744530141253559\n-0.5549323338240979\n-0.5742869072018928\n-0.5547477902783193\n-0.574122138705299\n-0.554564087691382\n-0.5739586807228186\n-0.5543812074809076\n-0.5737965059936514\n-0.5541991315805237\n-0.5736355878927291\n-0.5540178424232458\n-0.573475900414851\n-0.5538373229254484\n-0.5733174181591734\n-0.5536575564713997\n-0.5731601163140468\n-0.553478526898342\n-0.5730039706421911\n-0.553300218482095\n-0.5728489574662043\n-0.5531226159231608\n-0.5726950536543928\n-0.5529457043333161\n-0.5725422366069238\n-0.5527694692226697\n-0.5723904842422938\n-0.5525938964871684\n-0.5722397749840966\n-0.5524189723965383\n-0.5720900877481114\n-0.5522446835826398\n-0.5719414019296799\n-0.5520710170282255\n-0.5717936973913759\n-0.551897960056084\n-0.5716469544509828\n-0.5517255003185572\n-0.5715011538697395\n-0.5515536257874151\n-0.5713562768408792\n-0.5513823247440769\n-0.5712123049784463\n-0.5512115857701652\n-0.5710692203063839\n-0.5510413977383796\n-0.570927005247895\n-0.5508717498036834\n-0.5707856426150703\n-0.5507026313947834\n-0.5706451155987713\n-0.550534032205901\n-0.5705054077587793\n-0.5503659421888172\n-0.5703665030141957\n-0.5501983515451885\n-0.5702283856340805\n-0.5500312507191139\n-0.5700910402283492\n-0.5498646303899565\n-0.5699544517389008\n-0.5496984814653988\n-0.5698186054309836\n-0.5495327950747325\n-0.5696834868847894\n-0.5493675625623683\n-0.5695490819872716\n-0.5492027754815598\n-0.5694153769241952\n-0.5490384255883357\n-0.5692823581723846\n-0.5488745048356302\n-0.569150012492202\n-0.5487110053676052\n-0.5690183269202205\n-0.548547919514158\n-0.5688872887621058\n-0.54838523978561\n-0.568756885585692\n-0.5482229588675639\n-0.5686271052142531\n-0.5480610696159304\n-0.5684979357199627\n-0.5478995650521141\n"
    }
   ],
   "source": [
    "params = model(X_train,Y,learning_rate=0.00008 ,num_iter=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'W1': array([[ 0.63871731,  0.18514312, -0.05441474, -0.36012398],\n        [ 0.33926768,  0.21637485,  0.33493971,  0.26406609],\n        [ 0.3851536 ,  0.23043138,  0.82562265,  0.58571796],\n        [-0.30188424,  0.14120479,  0.6755822 ,  0.48570996]]),\n 'b1': array([[-0.01693608]]),\n 'W2': array([[-0.23815439,  0.04171465],\n        [ 0.18874219,  0.77651078]]),\n 'b2': array([[-0.21265552]]),\n 'W3': array([[ 0.17593384,  0.02513344,  0.09181268, ..., -0.05397898,\n         -0.06316123, -0.09736226],\n        [ 0.12704086, -0.0767136 , -0.07339245, ..., -0.11719095,\n          0.23269758,  0.17885624],\n        [-0.09424005, -0.11490346, -0.06593895, ...,  0.08816669,\n         -0.08447778,  0.03379587],\n        ...,\n        [ 0.07605848, -0.18909857,  0.06690752, ..., -0.21612535,\n         -0.08450556,  0.09306973],\n        [ 0.01843263, -0.14756718, -0.14758974, ...,  0.02983598,\n          0.10299665,  0.03538827],\n        [-0.07134238, -0.16076634,  0.05571729, ..., -0.1292647 ,\n          0.09460647, -0.07784738]]),\n 'b3': array([[-5.38462873e-04],\n        [ 6.35914026e-04],\n        [-5.02338680e-04],\n        [-6.18737480e-04],\n        [ 4.18187394e-04],\n        [-7.75710749e-04],\n        [-1.55081848e-04],\n        [ 4.58211945e-04],\n        [-3.37640407e-04],\n        [ 3.67462813e-04],\n        [-4.19864256e-06],\n        [-3.63083576e-04],\n        [ 1.61648258e-04],\n        [-2.63004397e-04],\n        [-1.09232948e-03],\n        [-8.87551919e-04],\n        [ 5.14173595e-04],\n        [-8.54126854e-04],\n        [-1.56682147e-03],\n        [ 2.02628616e-04],\n        [-1.66642826e-03],\n        [-1.14142806e-04],\n        [ 5.45752024e-04],\n        [-3.31753692e-05],\n        [-2.47793130e-04],\n        [-4.42060779e-04],\n        [ 1.25007578e-04],\n        [-1.24124167e-03],\n        [ 4.05375663e-04],\n        [-1.06303326e-03],\n        [ 3.68836432e-04],\n        [-1.93074425e-04],\n        [-1.92447716e-04],\n        [ 1.10288347e-04],\n        [ 1.15095686e-04],\n        [-1.12051075e-03],\n        [-1.50894224e-04],\n        [-5.56621084e-04],\n        [-1.31479982e-03],\n        [ 6.04859344e-04],\n        [-4.33187117e-04],\n        [ 2.80044248e-05],\n        [ 3.50409402e-06],\n        [ 3.79420466e-04],\n        [ 1.71077107e-04],\n        [-3.63400173e-04],\n        [ 1.12411736e-05],\n        [ 3.83008811e-04],\n        [-4.88409981e-05],\n        [ 6.94409034e-04]]),\n 'W4': array([[ 0.14889524, -0.24932016,  0.21895027,  0.26944495, -0.1332949 ,\n          0.22206347,  0.07259496, -0.23963636,  0.08583459, -0.12661117,\n         -0.02593629,  0.10835526, -0.06231492,  0.07018218,  0.27736029,\n          0.24597945, -0.11687113,  0.23952255,  0.38359301, -0.04859342,\n          0.37367898,  0.08465087, -0.1210524 ,  0.00830444,  0.09336411,\n          0.13090841, -0.26377279,  0.30306782, -0.21598152,  0.28778119,\n         -0.10076976,  0.24702545,  0.06615683, -0.0637973 , -0.04493612,\n          0.29697225, -0.34866945,  0.12331576,  0.31671599, -0.1773779 ,\n          0.13062902, -0.02551507,  0.02468488, -0.08910662, -0.06928682,\n          0.10381528, -0.00658792, -0.28804842,  0.02984069, -0.2869223 ]]),\n 'b4': array([[-0.0039317]])}"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Testing we generate new dataset \n",
    "X_test,Y_test = make_datasets() \n",
    "\n",
    "# normalize tha test dataset before predicting\n",
    "X_test = (X_test- X_test.min()) / (X_test.max() - X_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(X_test,params)  # test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1.] [1.]\n"
    }
   ],
   "source": [
    "print(Y_pred[254],Y_test[254])  # just for conformation work well or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.751\n"
    }
   ],
   "source": [
    "\n",
    "## for test accuracy \n",
    "from sklearn.metrics import accuracy_score \n",
    "acc = accuracy_score(Y_pred,Y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('myweb': venv)",
   "language": "python",
   "name": "python_defaultSpec_1599662544585"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}